# ğŸ§  Prompt Quality Evaluator (Hindi + English)

This project is a terminal-based tool that evaluates and compares two AI-generated responses (A vs B) for the same prompt. It records which response is better, your reasoning, and quality scores â€” and saves everything to a structured CSV file.

> ğŸ¯ Inspired by real-world LLM evaluation tasks like those at **Welocalize**, **Scale AI**, **Remotasks**, etc.

---

## ğŸŒŸ Features

- Supports both Hindi and English prompts
- Compares two AI-generated responses (A and B)
- Collects feedback: preference, reason, helpfulness, correctness, tone, safety
- Saves each entry immediately to a CSV file
- Works fully offline using [Ollama](https://ollama.com)

---

## ğŸ“¦ Requirements

- Python 3.8 or higher
- [Ollama](https://ollama.com) installed locally
- Model pulled locally (e.g. `llama3`)

Install Python dependencies:

```bash
pip install ollama pandas
```
## ğŸš€ How to Run

## âœ… Step 1: Clone the repository

```bash
git clone https://github.com/Sourabsb/prompt-quality-evaluator
cd prompt-quality-evaluator
```
## âœ… Step 2: Install Python dependencies

```bash
pip install ollama pandas
```
## âœ… Step 3: Pull the model using Ollama

```bash
ollama pull llama3
```

## âœ… Step 4: Run the evaluator script

```bash
python evaluator.py
```
## âœ… Step 5: Enter prompts and evaluate
Follow the prompts in the terminal:

ğŸ“ Enter a user prompt (Hindi or English)

ğŸ…°ï¸ View Response A and Response B

âœ… Choose the better one (A or B)

ğŸ’¬ Give your reason for the choice

ğŸ“Š Rate the responses:

Helpfulness (1â€“5)

Correctness (1â€“5)

Tone (1â€“5)

Any unsafe/biased content (Yes/No)

## âœ… Step 6: View the saved dataset
```bash
ollama_prompt_evaluation_dataset.csv
```

## ğŸ“‚ Project Structure

prompt-quality-evaluator/

â”œâ”€â”€ evaluator.py # Main evaluator script

â”œâ”€â”€ ollama_prompt_evaluation_dataset.csv # Output CSV (auto-generated)

â”œâ”€â”€ README.md # Project documentation

## ğŸ›¡ Disclaimer
This tool is built for educational and research purposes only. The responses and evaluations are generated using local models and are not intended to be used in production without human review.


## ğŸ™Œ Acknowledgements

- Thanks to the open-source community for inspiration on LLM evaluation tasks.
- Powered by [Ollama](https://ollama.com) for local large language model access.
- Built using [Python](https://python.org) and [Pandas](https://pandas.pydata.org/) for scripting and data handling.
- Prompt evaluation logic inspired by common practices in instruction tuning, response ranking, and alignment testing.
